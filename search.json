[
  {
    "objectID": "annexe.html",
    "href": "annexe.html",
    "title": "Annexe",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nimport sys\nfrom pathlib import Path\n\n# Ajout du chemin du dossier \"code\" où se trouve le fichier svm_gui.py\nsys.path.append(str(\"fichier_python\"))\n\n# Importation de la classe (remplace \"NomDeLaClasse\" par le nom de la classe réelle)\nfrom svm_gui import *\nfrom svm_source import *\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA\nfrom time import time\n\nscaler = StandardScaler()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')\n\n\n\n\n\niris = datasets.load_iris()\nX = iris.data\nX = scaler.fit_transform(X)\ny = iris.target\nX = X[y != 0, :2]\ny = y[y != 0]\n\n# split train test\nX, y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Modified to split the data\n\n# fit the model\nparameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\nclf_linear = GridSearchCV(SVC(), parameters, cv=5)  # Modified to use GridSearchCV for tuning C\nclf_linear.fit(X_train, y_train)\n\n# compute the score\nprint('Generalization score for linear kernel: %s, %s' %\n      (clf_linear.score(X_train, y_train),\n       clf_linear.score(X_test, y_test)))\n\n\n\n\n\n# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\n\nAvec ces paramètres, on trouve:\n\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\n\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "annexe.html#packages",
    "href": "annexe.html#packages",
    "title": "Annexe",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nimport sys\nfrom pathlib import Path\n\n# Ajout du chemin du dossier \"code\" où se trouve le fichier svm_gui.py\nsys.path.append(str(\"fichier_python\"))\n\n# Importation de la classe (remplace \"NomDeLaClasse\" par le nom de la classe réelle)\nfrom svm_gui import *\nfrom svm_source import *\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA\nfrom time import time\n\nscaler = StandardScaler()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')"
  },
  {
    "objectID": "annexe.html#question-1-kernel-linéaire",
    "href": "annexe.html#question-1-kernel-linéaire",
    "title": "Annexe",
    "section": "",
    "text": "iris = datasets.load_iris()\nX = iris.data\nX = scaler.fit_transform(X)\ny = iris.target\nX = X[y != 0, :2]\ny = y[y != 0]\n\n# split train test\nX, y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Modified to split the data\n\n# fit the model\nparameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\nclf_linear = GridSearchCV(SVC(), parameters, cv=5)  # Modified to use GridSearchCV for tuning C\nclf_linear.fit(X_train, y_train)\n\n# compute the score\nprint('Generalization score for linear kernel: %s, %s' %\n      (clf_linear.score(X_train, y_train),\n       clf_linear.score(X_test, y_test)))"
  },
  {
    "objectID": "annexe.html#question-2-kernel-polynomial",
    "href": "annexe.html#question-2-kernel-polynomial",
    "title": "Annexe",
    "section": "",
    "text": "# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\n\nAvec ces paramètres, on trouve:\n\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\n\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "annexe.html#question-4-influence-du-paramètre-de-régularisation",
    "href": "annexe.html#question-4-influence-du-paramètre-de-régularisation",
    "title": "Annexe",
    "section": "Question 4) Influence du paramètre de régularisation",
    "text": "Question 4) Influence du paramètre de régularisation\n\n# Extract features\n\n# features using only illuminations\nX = (np.mean(images, axis=3)).reshape(n_samples, -1)\n\n# # or compute features using colors (3 times more features)\n# X = images.copy().reshape(n_samples, -1)\n\n# Scale features\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)\n# Split data into a half training and half test set\n# X_train, X_test, y_train, y_test, images_train, images_test = \\\n#    train_test_split(X, y, images, test_size=0.5, random_state=0)\n# X_train, X_test, y_train, y_test = \\\n#    train_test_split(X, y, test_size=0.5, random_state=0)\n\nindices = np.random.permutation(X.shape[0])\ntrain_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\nX_train, X_test = X[train_idx, :], X[test_idx, :]\ny_train, y_test = y[train_idx], y[test_idx]\nimages_train, images_test = images[\n    train_idx, :, :, :], images[test_idx, :, :, :]\n\n\nprint(\"--- Linear kernel ---\")\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\n\n# fit a classifier (linear) and test all the Cs\nCs = 10. ** np.arange(-5, 6)\nscores = []\nfor C in Cs:\n    clf = SVC(kernel='linear', C=C)\n    clf.fit(X_train, y_train)\n    scores.append(clf.score(X_train, y_train)) \n\nind = np.argmax(scores)\nbest_C = Cs[ind]\nprint(\"Best C: {}\".format(Cs[ind]))\n\nplt.figure()\nplt.plot(Cs, scores)\nplt.xlabel(\"Parametres de regularisation C\")\nplt.ylabel(\"Scores d'apprentissage\")\nplt.xscale(\"log\")\nplt.tight_layout()\nplt.show()\nprint(\"Best score: {}\".format(np.max(scores)))\n\nprint(\"Predicting the people names on the testing set\")\nt0 = time()\n\nDans ce cas, le meilleur C est 1e-3 avec un score de 1. Pour des valeurs de C plus grandes, le score reste stable ce qui indique que les performances se stabilisent.\nMaintenant que nous avons identifié le meilleur C, nous réentraînons le modèle avec cette valeur, et nous effectuons des prédictions finales pour l’ensemble de test. Nous comparons ensuite la précision obtenue à un niveau de chance (prédiction aléatoire).\n\n# predict labels for the X_test images with the best classifier\nclf = SVC(kernel='linear', C=best_C)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"done in %0.3fs\" % (time() - t0))\n# The chance level is the accuracy that will be reached when constantly predicting the majority class.\nprint(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\nprint(\"Accuracy : %s\" % clf.score(X_test, y_test))\n\n\n# Qualitative evaluation of the predictions using matplotlib\n\nprediction_titles = [title(y_pred[i], y_test[i], names)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(images_test, prediction_titles)\nplt.show()\n# Look at the coefficients\nplt.figure()\nplt.imshow(np.reshape(clf.coef_, (h, w)))\nplt.show()"
  },
  {
    "objectID": "annexe.html#question-5-ajout-de-nuisance-sur-les-variables",
    "href": "annexe.html#question-5-ajout-de-nuisance-sur-les-variables",
    "title": "Annexe",
    "section": "Question 5) Ajout de nuisance sur les variables",
    "text": "Question 5) Ajout de nuisance sur les variables\n\ndef run_svm_cv(_X, _y):\n    _indices = np.random.permutation(_X.shape[0])\n    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n\n    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n    _svr = svm.SVC()\n    _clf_linear = GridSearchCV(_svr, _parameters)\n    _clf_linear.fit(_X_train, _y_train)\n\n    print('Generalization score for linear kernel: %s, %s \\n' %\n          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n\nprint(\"Score sans variable de nuisance\")\nrun_svm_cv(X, y) \n\nprint(\"Score avec variable de nuisance\")\nn_features = X.shape[1]\n# On rajoute des variables de nuisances\nsigma = 1\nnoise = sigma * np.random.randn(n_samples, 300, )\nX_noisy = np.concatenate((X, noise), axis=1)\nX_noisy = X_noisy[np.random.permutation(X.shape[0])]\nrun_svm_cv(X_noisy, y)"
  },
  {
    "objectID": "annexe.html#question-6-réduction-des-dimensions-pca",
    "href": "annexe.html#question-6-réduction-des-dimensions-pca",
    "title": "Annexe",
    "section": "Question 6) Réduction des dimensions PCA",
    "text": "Question 6) Réduction des dimensions PCA\n\nn_components = 80  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 80 composantes\")\nrun_svm_cv(X_pca, y) \nn_components = 120  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 120 composantes\")\nrun_svm_cv(X_pca, y)"
  }
]