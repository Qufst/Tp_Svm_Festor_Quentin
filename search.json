[
  {
    "objectID": "annexe.html",
    "href": "annexe.html",
    "title": "Annexe",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nimport sys\nfrom pathlib import Path\n\n# Ajout du chemin du dossier \"code\" où se trouve le fichier svm_gui.py\nsys.path.append(str(\"fichier_python\"))\n\n# Importation de la classe (remplace \"NomDeLaClasse\" par le nom de la classe réelle)\nfrom svm_gui import *\nfrom svm_source import *\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA\nfrom time import time\n\nscaler = StandardScaler()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')\n\n\n\n\n\niris = datasets.load_iris()\nX = iris.data\nX = scaler.fit_transform(X)\ny = iris.target\nX = X[y != 0, :2]\ny = y[y != 0]\n\n# split train test\nX, y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Modified to split the data\n\n# fit the model\nparameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\nclf_linear = GridSearchCV(SVC(), parameters, cv=5)  # Modified to use GridSearchCV for tuning C\nclf_linear.fit(X_train, y_train)\n\n# compute the score\nprint('Generalization score for linear kernel: %s, %s' %\n      (clf_linear.score(X_train, y_train),\n       clf_linear.score(X_test, y_test)))\n\n\n\n\n\n# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\n\nAvec ces paramètres, on trouve:\n\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\n\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "annexe.html#packages",
    "href": "annexe.html#packages",
    "title": "Annexe",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nimport sys\nfrom pathlib import Path\n\n# Ajout du chemin du dossier \"code\" où se trouve le fichier svm_gui.py\nsys.path.append(str(\"fichier_python\"))\n\n# Importation de la classe (remplace \"NomDeLaClasse\" par le nom de la classe réelle)\nfrom svm_gui import *\nfrom svm_source import *\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA\nfrom time import time\n\nscaler = StandardScaler()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')"
  },
  {
    "objectID": "annexe.html#question-1-kernel-linéaire",
    "href": "annexe.html#question-1-kernel-linéaire",
    "title": "Annexe",
    "section": "",
    "text": "iris = datasets.load_iris()\nX = iris.data\nX = scaler.fit_transform(X)\ny = iris.target\nX = X[y != 0, :2]\ny = y[y != 0]\n\n# split train test\nX, y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Modified to split the data\n\n# fit the model\nparameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\nclf_linear = GridSearchCV(SVC(), parameters, cv=5)  # Modified to use GridSearchCV for tuning C\nclf_linear.fit(X_train, y_train)\n\n# compute the score\nprint('Generalization score for linear kernel: %s, %s' %\n      (clf_linear.score(X_train, y_train),\n       clf_linear.score(X_test, y_test)))"
  },
  {
    "objectID": "annexe.html#question-2-kernel-polynomial",
    "href": "annexe.html#question-2-kernel-polynomial",
    "title": "Annexe",
    "section": "",
    "text": "# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\n\nAvec ces paramètres, on trouve:\n\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\n\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "annexe.html#question-4-influence-du-paramètre-de-régularisation",
    "href": "annexe.html#question-4-influence-du-paramètre-de-régularisation",
    "title": "Annexe",
    "section": "Question 4) Influence du paramètre de régularisation",
    "text": "Question 4) Influence du paramètre de régularisation\n\n# Extract features\n\n# features using only illuminations\nX = (np.mean(images, axis=3)).reshape(n_samples, -1)\n\n# # or compute features using colors (3 times more features)\n# X = images.copy().reshape(n_samples, -1)\n\n# Scale features\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)\n# Split data into a half training and half test set\n# X_train, X_test, y_train, y_test, images_train, images_test = \\\n#    train_test_split(X, y, images, test_size=0.5, random_state=0)\n# X_train, X_test, y_train, y_test = \\\n#    train_test_split(X, y, test_size=0.5, random_state=0)\n\nindices = np.random.permutation(X.shape[0])\ntrain_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\nX_train, X_test = X[train_idx, :], X[test_idx, :]\ny_train, y_test = y[train_idx], y[test_idx]\nimages_train, images_test = images[\n    train_idx, :, :, :], images[test_idx, :, :, :]\n\n\nprint(\"--- Linear kernel ---\")\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\n\n# fit a classifier (linear) and test all the Cs\nCs = 10. ** np.arange(-5, 6)\nscores = []\nfor C in Cs:\n    clf = SVC(kernel='linear', C=C)\n    clf.fit(X_train, y_train)\n    scores.append(clf.score(X_train, y_train)) \n\nind = np.argmax(scores)\nbest_C = Cs[ind]\nprint(\"Best C: {}\".format(Cs[ind]))\n\nplt.figure()\nplt.plot(Cs, scores)\nplt.xlabel(\"Parametres de regularisation C\")\nplt.ylabel(\"Scores d'apprentissage\")\nplt.xscale(\"log\")\nplt.tight_layout()\nplt.show()\nprint(\"Best score: {}\".format(np.max(scores)))\n\nprint(\"Predicting the people names on the testing set\")\nt0 = time()\n\nDans ce cas, le meilleur C est 1e-3 avec un score de 1. Pour des valeurs de C plus grandes, le score reste stable ce qui indique que les performances se stabilisent.\nMaintenant que nous avons identifié le meilleur C, nous réentraînons le modèle avec cette valeur, et nous effectuons des prédictions finales pour l’ensemble de test. Nous comparons ensuite la précision obtenue à un niveau de chance (prédiction aléatoire).\n\n# predict labels for the X_test images with the best classifier\nclf = SVC(kernel='linear', C=best_C)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"done in %0.3fs\" % (time() - t0))\n# The chance level is the accuracy that will be reached when constantly predicting the majority class.\nprint(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\nprint(\"Accuracy : %s\" % clf.score(X_test, y_test))\n\n\n# Qualitative evaluation of the predictions using matplotlib\n\nprediction_titles = [title(y_pred[i], y_test[i], names)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(images_test, prediction_titles)\nplt.show()\n# Look at the coefficients\nplt.figure()\nplt.imshow(np.reshape(clf.coef_, (h, w)))\nplt.show()"
  },
  {
    "objectID": "annexe.html#question-5-ajout-de-nuisance-sur-les-variables",
    "href": "annexe.html#question-5-ajout-de-nuisance-sur-les-variables",
    "title": "Annexe",
    "section": "Question 5) Ajout de nuisance sur les variables",
    "text": "Question 5) Ajout de nuisance sur les variables\n\ndef run_svm_cv(_X, _y):\n    _indices = np.random.permutation(_X.shape[0])\n    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n\n    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n    _svr = svm.SVC()\n    _clf_linear = GridSearchCV(_svr, _parameters)\n    _clf_linear.fit(_X_train, _y_train)\n\n    print('Generalization score for linear kernel: %s, %s \\n' %\n          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n\nprint(\"Score sans variable de nuisance\")\nrun_svm_cv(X, y) \n\nprint(\"Score avec variable de nuisance\")\nn_features = X.shape[1]\n# On rajoute des variables de nuisances\nsigma = 1\nnoise = sigma * np.random.randn(n_samples, 300, )\nX_noisy = np.concatenate((X, noise), axis=1)\nX_noisy = X_noisy[np.random.permutation(X.shape[0])]\nrun_svm_cv(X_noisy, y)"
  },
  {
    "objectID": "annexe.html#question-6-réduction-des-dimensions-pca",
    "href": "annexe.html#question-6-réduction-des-dimensions-pca",
    "title": "Annexe",
    "section": "Question 6) Réduction des dimensions PCA",
    "text": "Question 6) Réduction des dimensions PCA\n\nn_components = 80  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 80 composantes\")\nrun_svm_cv(X_pca, y) \nn_components = 120  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 120 composantes\")\nrun_svm_cv(X_pca, y)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tp3_Svm",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM, pour Support Vector Machines) sont une famille d’algorithmes d’apprentissage supervisé largement utilisés pour résoudre des problèmes de classification et de régression. Leur principe repose sur la recherche de la meilleure séparation possible entre différentes classes de données à l’aide d’hyperplans.\nComme énoncé précédemment, l’objectif principal d’un SVM est de trouver un hyperplan (une frontière) qui sépare les classes de manière optimale, en maximisant la marge entre les points les plus proches de chaque classe, appelés vecteurs de support. Ce type de modèle est particulièrement apprécié pour sa capacité à gérer des données complexes, voire non linéaires, grâce à l’utilisation de la méthode du noyau.\nCas linéaire : Si les données sont linéairement séparables, l’algorithme trouve un hyperplan qui sépare les classes en maximisant la distance entre les vecteurs de support des deux classes.\nCas non linéaire : Si les données ne peuvent pas être séparées par une simple ligne (ou un plan dans des dimensions plus élevées), les SVM utilisent des fonctions noyau (kernel functions) pour transformer les données dans un espace de plus haute dimension où une séparation linéaire devient possible. On a alors plusieurs possibilités, nous utiliserons principalement les noyaux linéaire, polynomiale et gaussien.\nDans ce tp, l’objectif sera d’explorer l’application des SVM en comparant les performances des modèles avec les différents noyaux et en faisant varier les hyperparamètres afin de comprendre les impacts de ces manipulations.\n#| echo: false #| eval: true import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC import sys from pathlib import Path\n# Ajout du chemin du dossier “code” où se trouve le fichier svm_gui.py sys.path.append(str(“fichier_python”))\n# Importation de la classe (remplace “NomDeLaClasse” par le nom de la classe réelle) from svm_gui import  from svm_source import  from sklearn import svm from sklearn import datasets from sklearn.utils import shuffle from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.datasets import fetch_lfw_people from sklearn.decomposition import PCA from time import time\nscaler = StandardScaler()\nimport warnings warnings.filterwarnings(“ignore”)\n\n\n\n\nPour ce compte rendu, il a été décidé de laisser le code en annèxe afin de ne pas gacher la rédaction. On n'affichera dans la partie principale seulement les résultats commentés ainsi que les graphiques, sauf si l'apparition du code semble pertinante pour la rédaction. \n\n# Mise en oeuvre\n\n## Question 1) Kernel linéaire\n\nNous reprenons le code donné en annexe concernant l'utilisation des SVM et l'appliquons à nos données Iris. Tout d'abord, nous classifions la classe 1 contre la classe 2 en utilisant uniquement les deux premières variables caractéristiques et un noyau linéaire. Les données sont ensuite séparées de manière équitable en deux échantillons : un pour l'entraînement et un pour le test, afin d'évaluer la qualité de notre modèle.\n\nNous évaluons ensuite la performance du modèle sur les données d'entraînement, puis sur les données de test, et obtenons les scores suivants :\n\n\n\n\n```{python}\n#| echo: false\n#| eval: true\niris = datasets.load_iris()\nX = iris.data\nX = scaler.fit_transform(X)\ny = iris.target\nX = X[y != 0, :2]\ny = y[y != 0]\n\n# split train test\nX, y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Modified to split the data\n\n# fit the model\nparameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\nclf_linear = GridSearchCV(SVC(), parameters, cv=5)  # Modified to use GridSearchCV for tuning C\nclf_linear.fit(X_train, y_train)\n\n# compute the score\nprint('Generalization score for linear kernel: %s, %s' %\n      (clf_linear.score(X_train, y_train),\n       clf_linear.score(X_test, y_test)))\n\nL’analyse de ces résultats montre que le modèle SVM avec un noyau linéaire a une précision de 76 % sur les données d’entraînement, ce qui indique une bonne adéquation initiale du modèle. Cependant, une baisse de performance est observée sur les données de test, avec une précision de 66 %, ce qui peut indiquer que le modèle a un léger problème de généralisation. Cela suggère que le modèle ne capture peut-être pas parfaitement la complexité des données, ou qu’il y a peut-être un léger surapprentissage sur l’ensemble d’entraînement. Ce résultat nous donne envie de de tester un autre noyau afin de vérifier si un noyau différent améliorerait la capacité de généralisation.\n\n\nAprès avoir observé des résultats mitigés avec le noyau linéaire, nous décidons d’utiliser un noyau polynomial et de comparer ses performances. Nous utilisons évidemment la même séparation des données pour garantir une comparaison juste entre les deux approches.\nLes paramètres optimaux trouvés avec la validation croisée sont:\n#| echo: false\n#| eval: true\n# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\nAvec ces paramètres, on trouve:\n#| echo: false\n#| eval: true\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\nLe modèle SVM avec un noyau polynomial donne une précision de 70 % sur l’ensemble d’entraînement et 68 % sur l’ensemble de test. Comparé au noyau linéaire, nous constatons donc une légère amélioration de la précision sur les données de test (de 66 % à 68 %), ce qui montre que le noyau polynomial capte légèrement mieux les relations dans les données, notamment des interactions non linéaires. Cependant, on observe également une diminution de la précision sur les données d’entraînement (de 76 % à 70 %), ce qui pourrait indiquer que le modèle polynomial est moins adapté à la structure linéaire des données.\nL’augmentation de la précision sur le test, bien que marginale, suggère que le modèle polynomial pourrait mieux généraliser dans ce contexte. Toutefois, l’amélioration n’est pas suffisamment significative pour justifier l’usage d’un noyau polynomial complexe dans ce cas particulier. Le noyau linéaire reste donc une option solide pour ce type de problème.\nLes graphiques ci-dessous illustrent les deux classifications des données de l’ensemble Iris utilisées ci-dessus.\n#| echo: false\n#| eval: true\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "index.html#question-2-kernel-polynomial",
    "href": "index.html#question-2-kernel-polynomial",
    "title": "Tp3_Svm",
    "section": "",
    "text": "Après avoir observé des résultats mitigés avec le noyau linéaire, nous décidons d’utiliser un noyau polynomial et de comparer ses performances. Nous utilisons évidemment la même séparation des données pour garantir une comparaison juste entre les deux approches.\nLes paramètres optimaux trouvés avec la validation croisée sont:\n#| echo: false\n#| eval: true\n# Q2 polynomial kernel\nCs = list(np.logspace(-3, 3, 5))\ngammas = 10. ** np.arange(1, 2)\ndegrees = np.r_[1, 2, 3]\n\nparameters = {'C': Cs, 'gamma': gammas, 'degree': degrees, 'kernel': ['poly']}\nclf_poly = GridSearchCV(SVC(),param_grid=parameters,n_jobs = -1 )  # Modified to use GridSearchCV for tuning C, gamma, and degree\nclf_poly.fit(X_train, y_train)\n\nprint(clf_poly.best_params_)\nAvec ces paramètres, on trouve:\n#| echo: false\n#| eval: true\nprint('Generalization score for polynomial kernel: %s, %s' %\n      (clf_poly.score(X_train, y_train),\n       clf_poly.score(X_test, y_test)))\nLe modèle SVM avec un noyau polynomial donne une précision de 70 % sur l’ensemble d’entraînement et 68 % sur l’ensemble de test. Comparé au noyau linéaire, nous constatons donc une légère amélioration de la précision sur les données de test (de 66 % à 68 %), ce qui montre que le noyau polynomial capte légèrement mieux les relations dans les données, notamment des interactions non linéaires. Cependant, on observe également une diminution de la précision sur les données d’entraînement (de 76 % à 70 %), ce qui pourrait indiquer que le modèle polynomial est moins adapté à la structure linéaire des données.\nL’augmentation de la précision sur le test, bien que marginale, suggère que le modèle polynomial pourrait mieux généraliser dans ce contexte. Toutefois, l’amélioration n’est pas suffisamment significative pour justifier l’usage d’un noyau polynomial complexe dans ce cas particulier. Le noyau linéaire reste donc une option solide pour ce type de problème.\nLes graphiques ci-dessous illustrent les deux classifications des données de l’ensemble Iris utilisées ci-dessus.\n#| echo: false\n#| eval: true\n\n# display your results using frontiere\n\ndef f_linear(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_linear.predict(xx.reshape(1, -1))\n\ndef f_poly(xx):\n    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n    return clf_poly.predict(xx.reshape(1, -1))\n\nplt.ion()\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplot_2d(X, y)\nplt.title(\"iris dataset\")\n\nplt.subplot(132)\nfrontiere(f_linear, X, y)\nplt.title(\"linear kernel\")\n\nplt.subplot(133)\nfrontiere(f_poly, X, y)\n\nplt.title(\"polynomial kernel\")\nplt.tight_layout()\nplt.draw()"
  },
  {
    "objectID": "index.html#question-4-influence-du-paramètre-de-régularisation",
    "href": "index.html#question-4-influence-du-paramètre-de-régularisation",
    "title": "Tp3_Svm",
    "section": "Question 4) Influence du paramètre de régularisation",
    "text": "Question 4) Influence du paramètre de régularisation\nComme précédemment, on sépare nos données en deux échantillons de tailles distinctes pour avoir un échantillon d’entrainement et un échantillon de test. On veut montrer l’influence du paramètre de régularisation. Pour ce faire, on décide d’afficher l’erreur de prédiction sur une échelle logarithmique entre 1e5 et 1e-5., et on obtient:\n#| echo: false\n#| eval: true\n\n# Extract features\n\n# features using only illuminations\nX = (np.mean(images, axis=3)).reshape(n_samples, -1)\n\n# # or compute features using colors (3 times more features)\n# X = images.copy().reshape(n_samples, -1)\n\n# Scale features\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)\n# Split data into a half training and half test set\n# X_train, X_test, y_train, y_test, images_train, images_test = \\\n#    train_test_split(X, y, images, test_size=0.5, random_state=0)\n# X_train, X_test, y_train, y_test = \\\n#    train_test_split(X, y, test_size=0.5, random_state=0)\n\nindices = np.random.permutation(X.shape[0])\ntrain_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\nX_train, X_test = X[train_idx, :], X[test_idx, :]\ny_train, y_test = y[train_idx], y[test_idx]\nimages_train, images_test = images[\n    train_idx, :, :, :], images[test_idx, :, :, :]\n\nLe meilleur paramètre C est celui qui donne le score de test le plus élevé.\n#| echo: false\n#| eval: true\nprint(\"--- Linear kernel ---\")\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\n\n# fit a classifier (linear) and test all the Cs\nCs = 10. ** np.arange(-5, 6)\nscores = []\nfor C in Cs:\n    clf = SVC(kernel='linear', C=C)\n    clf.fit(X_train, y_train)\n    scores.append(clf.score(X_train, y_train)) \n\nind = np.argmax(scores)\nbest_C = Cs[ind]\nprint(\"Best C: {}\".format(Cs[ind]))\n\nplt.figure()\nplt.plot(Cs, scores)\nplt.xlabel(\"Parametres de regularisation C\")\nplt.ylabel(\"Scores d'apprentissage\")\nplt.xscale(\"log\")\nplt.tight_layout()\nplt.show()\nprint(\"Best score: {}\".format(np.max(scores)))\n\nprint(\"Predicting the people names on the testing set\")\nt0 = time()\nDans ce cas, le meilleur C est 1e-3 avec un score de 1. Pour des valeurs de C plus grandes, le score reste stable ce qui indique que les performances se stabilisent.\nMaintenant que nous avons identifié le meilleur C, nous réentraînons le modèle avec cette valeur, et nous effectuons des prédictions finales pour l’ensemble de test. Nous comparons ensuite la précision obtenue à un niveau de chance (prédiction aléatoire).\n#| echo: false\n#| eval: true\n# predict labels for the X_test images with the best classifier\nclf = SVC(kernel='linear', C=best_C)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"done in %0.3fs\" % (time() - t0))\n# The chance level is the accuracy that will be reached when constantly predicting the majority class.\nprint(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\nprint(\"Accuracy : %s\" % clf.score(X_test, y_test))\nNous obtenons que la précision du modèle pour les données de test est de 0.89, alors que le taux de chance (niveau aléatoire) est de 0.62. Cela signifie que le modèle a une performance bien supérieure au hasard, ce qui indique qu’il a bien appris à distinguer les classes à partir des caractéristiques fournies.\nL’échantillon d’images du premier document ci-dessous illustre les résultats de l’algorithme. La deuxième image obtenue montre les coefficients du modèle linéaire appris sous la forme d’une carte de chaleur. Cela permet de visualiser quelles parties des images sont les plus importantes pour le classifieur afin de prendre ses décisions. Ces zones correspondent à des caractéristiques importantes identifiées par le modèle dans les images.\n#| echo: false\n#| eval: true\n# Qualitative evaluation of the predictions using matplotlib\n\nprediction_titles = [title(y_pred[i], y_test[i], names)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(images_test, prediction_titles)\nplt.show()\n# Look at the coefficients\nplt.figure()\nplt.imshow(np.reshape(clf.coef_, (h, w)))\nplt.show()\nLes zones importantes pour la prédiction semblent être l’implantation des cheveux au niveau du front, lenez, les yeux, ainsi que la bouche et la forme du crâne."
  },
  {
    "objectID": "index.html#question-5-ajout-de-nuisance-sur-les-variables",
    "href": "index.html#question-5-ajout-de-nuisance-sur-les-variables",
    "title": "Tp3_Svm",
    "section": "Question 5) Ajout de nuisance sur les variables",
    "text": "Question 5) Ajout de nuisance sur les variables\nAjouter des variables de nuisance dans les données d’apprentissage est une méthode pour évaluer la robustesse et la capacité de généralisation d’un modèle de classification. Les variables de nuisance sont des caractéristiques ajoutées artificiellement qui n’ont aucune relation avec la variable cible. Il peut y avoir plusieurs objectifs à cet ajout. L’objectif principal est d’examiner l’impact de ces variables sur la performance du modèle et de vérifier si le modèle peut encore faire des prédictions précises malgré la présence de ces informations non pertinentes.\n#| echo: false\n#| eval: true\ndef run_svm_cv(_X, _y):\n    _indices = np.random.permutation(_X.shape[0])\n    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n\n    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n    _svr = svm.SVC()\n    _clf_linear = GridSearchCV(_svr, _parameters)\n    _clf_linear.fit(_X_train, _y_train)\n\n    print('Generalization score for linear kernel: %s, %s \\n' %\n          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n\nprint(\"Score sans variable de nuisance\")\nrun_svm_cv(X, y) \n\nprint(\"Score avec variable de nuisance\")\nn_features = X.shape[1]\n# On rajoute des variables de nuisances\nsigma = 1\nnoise = sigma * np.random.randn(n_samples, 300, )\nX_noisy = np.concatenate((X, noise), axis=1)\nX_noisy = X_noisy[np.random.permutation(X.shape[0])]\nrun_svm_cv(X_noisy, y) \nSans ajout de variables de nuisance, le modèle atteint une précision parfaite de 1.0 sur l’ensemble d’entraînement, et une précision de 0.9 sur l’ensemble de test. Cela indique que le modèle est très performant et capable de bien généraliser sur des données non vues.\nLorsque des variables de nuisance sont ajoutées, la précision sur l’ensemble de test chute considérablement à environ 0.51, près du niveau de chance pour ce problème (0.62). Cependant, la précision sur l’ensemble d’entraînement reste à 1.0. Ce résultat montre que, bien que le modèle continue à bien performer sur les données d’entraînement, il est moins capable de généraliser sur de nouvelles données lorsqu’il est confronté à des informations non pertinentes."
  },
  {
    "objectID": "index.html#question-6-réduction-des-dimensions-pca",
    "href": "index.html#question-6-réduction-des-dimensions-pca",
    "title": "Tp3_Svm",
    "section": "Question 6) Réduction des dimensions PCA",
    "text": "Question 6) Réduction des dimensions PCA\nAprès avoir examiné l’impact des variables de nuisance sur la performance du modèle, il est pertinent de se pencher sur une autre technique importante de prétraitement des données : la réduction de dimensions. En utilisant l’Analyse en Composantes Principales (PCA), nous cherchons à réduire le nombre de caractéristiques tout en conservant autant d’information que possible. Cette méthode peut aider à améliorer la performance du modèle en éliminant le bruit et en simplifiant la représentation des données. Nous allons donc appliquer la PCA sur les données perturbées par le bruit et évaluer comment cette réduction de dimension influence la précision du modèle SVM. On essaie une fois avec 20 composantes, une fois avec 80, et une fois avec 120.\n#| echo: false\n#| eval: true\n\nn_components = 80  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 80 composantes\")\nrun_svm_cv(X_pca, y) \nn_components = 120  # jouer avec ce parametre\npca = PCA(n_components=n_components).fit(X_noisy)\nX_pca = pca.transform(X_noisy)\nprint(\"Score apres reduction de dimension PCA pour 120 composantes\")\nrun_svm_cv(X_pca, y) \nOn remarque qu’avec 20 composantes les calculs sont trop longs, de même pour tout nombre inférieur à 80, donc on se contente de 80, et 120. On observe que moins on garde de composantes, plus le score remonte. Cependant il remonte faiblement, probablement car il faut diminuer largement le nombre de composantes pour obtenir un score bon, ce qui nécessite beaucoup de temps de calcul."
  }
]